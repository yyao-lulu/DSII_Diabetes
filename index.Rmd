---
title: "Data Science II Final Project"
author: "Yangwei Yan (yy2828), Yunqiu Yao (yy2827), Boxuan Li (bl2689)"
date: "4/18/2018"
output: 
  html_document:
    code_folding: hide
---

In real life, it is very important to know if a patient will be readmitted in some hospital due to some particular diseases. It is not only because readmission indicates that the patient needs to suffer more from the disease, also a potential reminder that the treatment used previously to treat the patient is not effective enough for the hospital. Generally, readmission should be avoided, which is the major motivation for this study. In this case, a dataset about the readmission status of patients with diabetes extracted from the hospital records, including information on the patient's demographic characteristics, the diagnosis and treatment, is analyzed to explore the impact of those features on the incidence of readmission in the population. This report includes the data description, explanatory study and supervised analysis, which serve to provide sense of the data and relationship among variables in whole.  The result shows that there is assication between serveral covatiates and the readmission occurrence, e.g., (!!!!!!!!!!!!!!!!!). 

```{r R set-up, include=FALSE}
knitr::opts_chunk$set(
  collapse = FALSE,
  warning = FALSE,
  message = FALSE,
  fig.width = 8,
  fig.asp = 1,
  out.width = "90%"
)

library(MASS)
library(tidyverse)
library(janitor)
library(ggridges)
library(ggthemes)
library(class)
library(caret)

theme_set(theme_bw())
theme_update(legend.position = "bottom")
```

```{r dataset cleaning and manipulation}
diabetes <- read_csv('./data/diabetic_data.csv') %>%
  clean_names()

# missing value proportion
diabetes[diabetes=="?"] = NA
sapply(diabetes,function(x) sum(is.na(x))/dim(diabetes)[1]) %>% .[.!=0]

# Therefore, we consider to omit those variables with large number of missing values, i.e., 'weight', 'payer code' and 'medical specialty'. We further omit NA values in other variables.
diabetes <- diabetes %>%
  select(., everything(), -weight, -payer_code, -medical_specialty) %>%
  na.omit() %>%
  arrange(patient_nbr) %>% 
  group_by(patient_nbr) %>% 
  filter(row_number(encounter_id)==1,
         !(discharge_disposition_id %in% c(11,13,14,19:21))) %>% 
# Classify the readmitted status into “Yes” if the patient was readmitted in less than 30 days and “No” if the patient was readmitted in more than 30 days or no record of readmission. 
  ungroup() %>% 
  mutate(readmitted=ifelse(readmitted=="<30","Yes","No")) %>% 
  filter(nateglinide %in% c("No","Steady"),
         glyburide_metformin %in% c("No","Steady"),
         gender %in% c("Female","Male"),
         acarbose %in% c("No","Steady"))
# Specify the variable "Diagnosis" as the correspnding diagnosed diseases
diabetes_tidy = diabetes %>% 
  mutate(
    diag_1=ifelse(diag_1>=390 & diag_1 <= 459 | diag_1 == 785, "Circulatory", 
                  ifelse(diag_1>=460 & diag_1 <= 519 | diag_1 == 786, "Respiratory", 
                  ifelse(diag_1>=520 & diag_1 <= 579 | diag_1 == 787, "Digestive",
                  ifelse(substr(diag_1, 1, 3) == 250, "Diabetes", 
                  ifelse(diag_1>=800 & diag_1 <= 999, "Injury",
                  ifelse(diag_1>=710 & diag_1 <= 739, "Musculoskeletal",
                  ifelse(diag_1>=580 & diag_1 <= 629 | diag_1 == 788, "Genitourinary",
                  ifelse(diag_1>=140 & diag_1 <= 239, "Neoplasms", "Other")))))))),
    diag_2=ifelse(diag_2>=390 & diag_2 <= 459 | diag_2 == 785, "Circulatory", 
                  ifelse(diag_2>=460 & diag_2 <= 519 | diag_2 == 786, "Respiratory", 
                  ifelse(diag_2>=520 & diag_2 <= 579 | diag_2 == 787, "Digestive",
                  ifelse(substr(diag_2, 1, 3) == 250, "Diabetes", 
                  ifelse(diag_2>=800 & diag_2 <= 999, "Injury",
                  ifelse(diag_2>=710 & diag_2 <= 739, "Musculoskeletal",
                  ifelse(diag_2>=580 & diag_2 <= 629 | diag_2 == 788, "Genitourinary",
                  ifelse(diag_2>=140 & diag_2 <= 239, "Neoplasms", "Other")))))))),
    diag_3=ifelse(diag_3>=390 & diag_3 <= 459 | diag_3 == 785, "Circulatory", 
                  ifelse(diag_3>=460 & diag_3 <= 519 | diag_3 == 786, "Respiratory", 
                  ifelse(diag_3>=520 & diag_3 <= 579 | diag_3 == 787, "Digestive",
                  ifelse(substr(diag_3, 1, 3) == 250, "Diabetes", 
                  ifelse(diag_3>=800 & diag_3 <= 999, "Injury",
                  ifelse(diag_3>=710 & diag_3 <= 739, "Musculoskeletal",
                  ifelse(diag_3>=580 & diag_3 <= 629 | diag_3 == 788, "Genitourinary",
                  ifelse(diag_3>=140 & diag_3 <= 239, "Neoplasms", "Other")))))))),
  ) 
diabetes_tidy = diabetes_tidy %>% 
  mutate(
    discharge = ifelse(discharge_disposition_id==1, "Home", "Other"),
    admission_source = ifelse(admission_source_id==7, "Emergency",
                  ifelse(admission_source_id %in% 1:3, "Referral", "Other"))
  ) %>% 
  select(-c(encounter_id,patient_nbr,admission_type_id,admission_source_id,discharge_disposition_id,chlorpropamide,acetohexamide,tolbutamide,miglitol,troglitazone,tolazamide,examide,citoglipton,glipizide_metformin,glimepiride_pioglitazone,metformin_rosiglitazone,metformin_pioglitazone)) %>%
  mutate_if(is.character, as.factor)

```

## Data Description
The dataset used in this study was extracted from the Health Facts database (Cerner Corporation, Kansas City, MO), a national data warehouse that collects comprehensive clinical records across hospitals throughout the United States. Information in the dataset was systematically collected from participating institutions electronic medical records. In this case, we focus on the records on "diabetic" encounters. The dataset contains `r nrow(diabetes)` observations and `r ncol(diabetes)` variables before data cleaning. Specifically, it incorporates basic information on each "diabetic" encounter for patients, including several demographic characteristics (i.e., "race", "gender" and "age"), hospital records about diagnosis, medical treatments, and th readmission status for each patient. Most of variables in the dataset are categorical, with two or three categories (e.g., "Yes", "No", "Steady", etc.), indicating whether the patient received some treatments or had some particular features. Information in this dataset can be helpful to evaluate the efficacy of different treatments to reduce the readmission rate of patients due to diabetes. Therefore, the "readmitted" variable is regarded as the main response in the supervised analysis section, which will aim to explore the impact of different treatments to the readmission due to diaebtes. Result of this research is promising to provide some insights in improvement of the diabetes treatment. 

During the data cleaning process, we omitted the variables with too many missing values with the proportion of missing values over 30%, i.e., 'weight', 'payer code' and 'medical specialty'. Subsequently, few missing values left in other variables were omiited as well. Data in the original dataset are considered to be correlated because it contained multiple visits per patients. Thus we used only the first encounter for each patient as the primary admission status. In addtion, we combined "No" and ">30" categories in the "readmitted" variable into ">30" category because it has been verified by research that it is more likely for a patient to readmit after 30 days due to his or her own healthy issues instead of the treatment. Therefore, only readmission within 30 days after discharge was considered to be associated with the treatment in this case, which is why three categories were combined into two indicating readmission related to treatment and readmission irrelevant to treatment respectively. In terms of those numeric variables referring to categorical meanings such as "diag_1", "diag_2", "diag_3" and "discharge", we substituted them with the original implications as factors. Futhermore, multiple variables were found to give extremely separate categories, i.e., the ratio of two categories is less than 0.1%, thus they were also removed and will not be considered in further analyses. Simultaneously, considering the overwhelming data that could cause difficulty in techinics used in supervised analysis, we decided to appropriately reduce the size to 20000 observations randomly.  After the process of data cleaning, the dataset still contains `r nrow(diabetes_tidy)` observations and `r ncol(diabetes_tidy)` variables.

More detailed information can be found in [here](ref:).

## Explanatory Data Analysis
```{r}
# summary table for numeric variables
diabetes_tidy %>% 
  select_if(is.integer) %>% 
  apply(2,summary) %>% 
  pander::pander(caption="For numeric variables")

# summary table for categorical variables
freq.table = function(x,name){
  table = data.frame(x)
  names(table) = c("Value","Count")
  table$Fraction = with(table,Count/sum(Count))
  data.frame(Variable=name,table)
}

fct_diabetes = diabetes_tidy %>%
  select_if(is.factor) %>% 
  lapply(table)

do.call(rbind,lapply(seq_along(fct_diabetes),function(i) freq.table(fct_diabetes[i],names(fct_diabetes[i])))) %>% 
  pander::pander(caption="For categorical variables")
```

The summary and the table presents a basic but comprehensive overview of the dataset, including the class of each variable, the proportion in the population for categorical variables and the related statitics for numeric variables. Based on the table, it can be found that there are 8 continuous and 24 categorical variables after data cleaning. Most patients are caucasian. Remarkably, older people seem to have higher risk for diabetes since the age of patients mostly falls around 70. The mean number of medications prescribed for those patients is about 16, among which insulin is still the major treatment medication since over 50% patients take insulin while the proportion of patients who take other medication is so small. Concerning the response "readmitted", only 10% of all patients studied in this case readmit within 30 days after previous discharge. Even though it is a minor fraction, it is still essential to study how to improve the treatment to avoid readmission. 

```{r}
# visualize the associations
# readmitted and age
age = diabetes_tidy %>%
  group_by(age) %>% 
  summarise(count_yes = sum(readmitted == "Yes"),
            count_no = sum(readmitted == "No"),
            proportion = count_yes/(count_yes + count_no))

ggplot(age, aes(age, proportion, group = 1)) + 
  geom_point() +
  geom_line() +
  labs(title = "The Association Between Readmitted Distribution and Age",
       x = "Age",
       y = "Readmitted Proportion")

# readmitted and primary diagnosis
disease = diabetes_tidy %>%
  group_by(diag_1) %>% 
  summarise(count_yes = sum(readmitted == "Yes"),
            count_no = sum(readmitted == "No"),
            proportion = count_yes/(count_yes + count_no))

overall_prop = sum(disease$count_yes)/(sum(disease$count_yes) + sum(disease$count_no))
ggplot(disease, aes(proportion, diag_1)) +
  geom_point() +
  geom_vline(xintercept = overall_prop, color = "red") + 
  labs(title = "The Association Between Readmitted Distribution and Diagnosed Disease",
       x = "Readmitted Proportion",
       y = "Primary Diagnosis")
```

By exploring the association between readmission proportion and some potential predictors, we obtained some interesting findings. As shown in the first plot above, the readmitted proportion increased dramatically in the age interval 0-30, then it almost remained stable in the age interval 30-60, with a following trend of increase in 60-90. It indicated that older diabete patients (above 60) tended to have a higher probability of being readmitted in 30 days. However, for 90+ years old patients, they had a relatively lower readmitted proportion compared to those with the age of 80-90. It could be possible that the patients with the age of 90+ were more likely to be expired due to other diseases before readmission.
&nbsp;

The second plot demonstrated the relationship between readmission proportion and primary diagnosis. The red vertical line is the reference line, representing the average proportion of readmitted. The readmitted proportions of patients diagnosed with neoplasms, genitourinary and other diseases were around the reference line, which indicated that these diseases might not have a strong relationship with readmission. In contrast, patients diagnosed with respiratory and injury could have a strong relationship with readmission status.

```{r sample data}
set.seed(100)
rsample = sample(1:nrow(diabetes_tidy), 20000)
diabetes_tidy = diabetes_tidy[rsample, ]
```


## Supervised Analysis

Classification techniques including logistic regression, k-nearest neighbor classifiers, tree-based methods and support vector machines were applied to predict whether a patient would be readmitted in 30 days. By comparing the performance of different techniques, we explored the association between readmission and potential predictors, and compared the prediction accuracy of each technique. 

```{r training set}
# create test set
set.seed(1)
nrow_train = sample(1:nrow(diabetes_tidy), 10000)
```

#### K-nearest neighbor (KNN) classifiers
K-nearest neighbor (KNN) classifiers is one of the model-free classification methods. KNN requires no assumption on the distrbution that generate the data. Thus we applied KNN in our dataset with large amounts of predictors. Since KNN measures the nearest distance between training points and the point $x_0$ to predict, it is impossible to evaluate the distance if the predictor $x_i$ is categorical. For example, it is not rationale to measure the distance between $x_0$ and "female" or $x_0$ and "male". Therefore, we only included numerical predictors to construct the KNN model.

```{r KNN}
# training and test set for knn
readmit = diabetes_tidy[nrow_train,]$readmitted
train = as.data.frame(diabetes_tidy[nrow_train, c(4:10, 14)])
test = diabetes_tidy[-nrow_train, c(4:10, 14)]

# perform KNN
set.seed(1)
pred_knn_prob = 1-attributes(knn(train, test, prob=TRUE,cl = readmit, k = 10))$prob
pred_knn = knn(train, test, cl = readmit, k = 10)
cm.knn = confusionMatrix(pred_knn, diabetes_tidy[-nrow_train,]$readmitted); cm.knn
```

#### Classification Tree
We fitted a classification tree to the training data and used cross-validation on the training set to determine the optimal tree size by chosing the optimal parameter cp.
```{r classification tree}
set.seed(1)
train_tree = diabetes_tidy[nrow_train,]
fit_tree = train(train_tree[,-30],
                 train_tree$readmitted,
                 method = "rpart",
                 trControl = trainControl(method = "cv", number = 10))
plot(fit_tree)
pred_tree_prob = predict(fit_tree, newdata = diabetes_tidy[-nrow_train,],type="prob")
pred_tree = predict(fit_tree, newdata = diabetes_tidy[-nrow_train,])
cm.tree = confusionMatrix(pred_tree, diabetes_tidy[-nrow_train,]$readmitted); cm.tree
```

#### Random Forest

Random forest technique was also used in this case. This method can not only predict the class of each observation in the test set, also measure the importance of variables during the fitting process. The number of variables randomly sampled as candidates at each split in this study was selected as the squre root of number of covariates used to fit the model, i.e., $\sqrt{32-1}\approx6$.

```{r randomforest}
set.seed(1)
library(randomForest)

diabetes_rf <- mutate(diabetes_tidy, readmitted = ifelse(readmitted =='Yes', 1,0)) %>%
  ungroup()

fit_rf = randomForest(readmitted~., data = diabetes_rf, subset = nrow_train, mtry = 6, importance = TRUE)
importance(fit_rf)
pred_rf_prob = predict(fit_rf, newdata=diabetes_tidy[-nrow_train,],type="response")
pred_rf = rep("No",length(pred_rf_prob))
pred_rf[pred_rf_prob>=0.3] = "Yes"
pred_rf = as.factor(pred_rf)
cm.rf = confusionMatrix(pred_rf, diabetes_tidy[-nrow_train,]$readmitted); cm.rf
```

The plot of "%IncMSE" and "IncNodePurity" presents the impact of each variable on the change of MSE and node purity in each split during the model fitting. %IncMSE is the most robust and informative measure. It is the increase in mse of predictions(estimated with out-of-bag-CV) as a result of variable j being permuted(values randomly shuffled). And the IncNodePurity relates to the loss function which by best splits are chosen. The larger the two values are, the more important the variable is. Hence, both plots indicate that several variables, e.g., "num_medication", "time_in_hospital", "number_lab_procedures",etc., are the most important to accurately predict the response "readmitted". 

After predicting the response in test set, we compared the predicted ones with the true reponse and made the confusion matrix. The test error rate is therefore $(1-0.9014)*100%=9.99%$.


#### Support Vector Machines
```{r svm}
library(e1071)

# model construction
svm.model = svm(readmitted~.,data=diabetes_tidy[nrow_train,],probability=TRUE,type="C-classification",cost=10,gamma=0.01)

pred_svm_prob = attributes(predict(svm.model,newdata=diabetes_tidy[-nrow_train,],probability=TRUE))$probabilities

pred_svm = predict(svm.model,newdata=diabetes_tidy[-nrow_train,])

cm.svm = confusionMatrix(pred_svm,diabetes_tidy[-nrow_train,]$readmitted); cm.svm

```
Here we tried to use support vector machine to perform the classification. In order to capture the potential nonlinearities in the support-vector classifiers and to obtain better prediction, we use the radial kernel for svm. The result shows that we have a low overall error rate (9.62%) for the prediction. The sensitivity is extremely high, with a value 99.96%, while the specificity is very low, which is only 0.21%. This means the svm model performs quite well on identifing those who do not need to be readmitted (true negative), but has poor ability on the identification of those who require readmission (true positive).

#### Logistic Regression
```{r logistic}
log.fit = glm(readmitted~.,data=diabetes_tidy,subset=nrow_train,family="binomial")
# contrasts(diabetes_tidy$readmitted)
pred.log.prob = predict(log.fit,newdata=diabetes_tidy[-nrow_train,],type="response")
pred_log = rep("No",length(pred.log.prob))
pred_log[pred.log.prob>=0.3] = "Yes"
cm.log = confusionMatrix(pred_log,diabetes_tidy[-nrow_train,]$readmitted); cm.log
```
Logistic regression is also a common way to address the classification problems. It can be seen from the table above that the overall error of the logistic model for presiction is very low, at around 10%. But similar to the SVM model, the logistic model also has a high sensitivity but a low specificity. Therefore, the model is also more suitable for the identification of those who are not necessarily to be re-admitted. 

```{r}
# roc curves
library(pROC)
roc.knn = roc(diabetes_tidy[-nrow_train,]$readmitted,pred_knn_prob,levels=c("No","Yes"))
auc.knn = roc.knn$auc
roc.tree = roc(diabetes_tidy[-nrow_train,]$readmitted,pred_tree_prob[,2],levels=c("No","Yes"))
auc.tree = roc.tree$auc
roc.rf = roc(diabetes_tidy[-nrow_train,]$readmitted,pred_rf_prob,levels=c("No","Yes"))
auc.rf = roc.rf$auc
roc.svm = roc(diabetes_tidy[-nrow_train,]$readmitted,pred_svm_prob[,2],levels=c("No","Yes"))
auc.svm = roc.svm$auc
roc.log = roc(diabetes_tidy[-nrow_train,]$readmitted,pred.log.prob,levels=c("No","Yes"))
auc.log = roc.log$auc

# roc plot
plot(roc.knn,legacy.axes=TRUE,col="red")
plot(roc.tree,add=TRUE,col="orange")
plot(roc.rf,add=TRUE,col="maroon")
plot(roc.svm,add=TRUE,col="yellow")
plot(roc.log,add=TRUE,col="brown")

# summary table
method = c("KNN","Classification Tree","Random Forest","SVM","Logistic")
auc = round(c(auc.knn,auc.tree,auc.rf,auc.svm,auc.log),3)
error = round(1-c(cm.knn$overall[[1]],cm.tree$overall[[1]],cm.rf$overall[[1]],cm.svm$overall[[1]],cm.log$overall[[1]]),5)
sens = round(c(cm.knn$byClass[[1]],cm.tree$byClass[[1]],cm.rf$byClass[[1]],cm.svm$byClass[[1]],cm.log$byClass[[1]]),6)
spec = round(c(cm.knn$byClass[[2]],cm.tree$byClass[[2]],cm.rf$byClass[[2]],cm.svm$byClass[[2]],cm.log$byClass[[2]]),6)
comp = cbind(method,auc,error,sens,spec)
colnames(comp) = c("Method","AUC","Error Rate","Sensitivity","Specificity")
pander::pander(comp)

```

